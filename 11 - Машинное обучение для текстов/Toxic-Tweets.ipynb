{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Содержание<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Введение\" data-toc-modified-id=\"Введение-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Введение</a></span></li><li><span><a href=\"#Подготовка\" data-toc-modified-id=\"Подготовка-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Подготовка</a></span><ul class=\"toc-item\"><li><span><a href=\"#Проверка-типов-и-пропусков\" data-toc-modified-id=\"Проверка-типов-и-пропусков-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Проверка типов и пропусков</a></span></li><li><span><a href=\"#Проверка-дубликатов\" data-toc-modified-id=\"Проверка-дубликатов-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Проверка дубликатов</a></span></li></ul></li><li><span><a href=\"#Подготовка\" data-toc-modified-id=\"Подготовка-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Подготовка</a></span></li><li><span><a href=\"#Обучение\" data-toc-modified-id=\"Обучение-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Обучение</a></span></li><li><span><a href=\"#Вывод\" data-toc-modified-id=\"Вывод-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Вывод</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Введение\n",
    "Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/anton/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/anton/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/anton/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "pd.set_option('display.max_colwidth', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Подготовка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = pd.read_csv(\"toxic_comments.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm seemingly stuck with. Thanks.  (talk) 21:51, January 11, 2016 (UTC)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. He seems to care more about the formatting than the actual info.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on improvement - I wondered if the section statistics should be later on, or a subsection of \"\"types of accidents\"\"  -I think the references may need tidying so that they are all in the exact same format ie date format etc. I can do that later on, if no-one else does first - if you have any preferences for formatting style on references or want to do it yourself please let me know.\\n\\nThere appears to be a backlog on articles for review so I guess there may be a delay until a reviewer turns up. It's listed in the relevant form eg Wikipedia:Good_article_nominations#Transport  \"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember what page that's on?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\"\\n\\nCongratulations from me as well, use the tools well.  · talk \"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Your vandalism to the Matt Shirvington article has been reverted.  Please don't do it again, or you will be banned.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sorry if the word 'nonsense' was offensive to you. Anyway, I'm not intending to write anything in the article(wow they would jump on me for vandalism), I'm merely requesting that it be more encyclopedic so one can use it for school as a reference. I have been to the selective breeding page but it's almost a stub. It points to 'animal breeding' which is a short messy article that gives you no info. There must be someone around with expertise in eugenics? 93.161.107.169</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>alignment on this subject and which are contrary to those of DuLithgow</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 text  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                           Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    D'aww! He matches this background colour I'm seemingly stuck with. Thanks.  (talk) 21:51, January 11, 2016 (UTC)   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                           Hey man, I'm really not trying to edit war. It's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. He seems to care more about the formatting than the actual info.   \n",
       "3  \"\\nMore\\nI can't make any real suggestions on improvement - I wondered if the section statistics should be later on, or a subsection of \"\"types of accidents\"\"  -I think the references may need tidying so that they are all in the exact same format ie date format etc. I can do that later on, if no-one else does first - if you have any preferences for formatting style on references or want to do it yourself please let me know.\\n\\nThere appears to be a backlog on articles for review so I guess there may be a delay until a reviewer turns up. It's listed in the relevant form eg Wikipedia:Good_article_nominations#Transport  \"   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 You, sir, are my hero. Any chance you remember what page that's on?   \n",
       "5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \"\\n\\nCongratulations from me as well, use the tools well.  · talk \"   \n",
       "6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK   \n",
       "7                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Your vandalism to the Matt Shirvington article has been reverted.  Please don't do it again, or you will be banned.   \n",
       "8                                                                                                                                                            Sorry if the word 'nonsense' was offensive to you. Anyway, I'm not intending to write anything in the article(wow they would jump on me for vandalism), I'm merely requesting that it be more encyclopedic so one can use it for school as a reference. I have been to the selective breeding page but it's almost a stub. It points to 'animal breeding' which is a short messy article that gives you no info. There must be someone around with expertise in eugenics? 93.161.107.169   \n",
       "9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              alignment on this subject and which are contrary to those of DuLithgow   \n",
       "\n",
       "   toxic  \n",
       "0      0  \n",
       "1      0  \n",
       "2      0  \n",
       "3      0  \n",
       "4      0  \n",
       "5      0  \n",
       "6      1  \n",
       "7      0  \n",
       "8      0  \n",
       "9      0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Проверка типов и пропусков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159571 entries, 0 to 159570\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   text    159571 non-null  object\n",
      " 1   toxic   159571 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 2.4+ MB\n"
     ]
    }
   ],
   "source": [
    "frame.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Проверка дубликатов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество дубликатов: 0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Количество дубликатов: {frame.duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подготовка\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class toxic_classification():\n",
    "    \"\"\"Класс реализующий все этапы предобработки и классификацию комментариев\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 models_and_params: list,\n",
    "                 score: str,\n",
    "                 solvers: list,\n",
    "                 stop_words,\n",
    "                 start_frame,\n",
    "                 target_column,\n",
    "                 data_column):\n",
    "        # Инициализация всех переданных параметров\n",
    "        self.models_and_params = models_and_params\n",
    "        self.score = score\n",
    "        self.solvers = solvers\n",
    "        self.stop_words = stop_words\n",
    "        self.start_frame = start_frame\n",
    "        self.target_column = target_column\n",
    "        self.data_column = data_column\n",
    "        self.lemm_corpus = None\n",
    "        self.splited_data = None\n",
    "        self.vect = None\n",
    "        self.max_score = -1\n",
    "        self.best_model = None\n",
    "        \n",
    "        # preprocessing\n",
    "        self.__text_clearning()\n",
    "        print(\"Первый этап пройден\")\n",
    "        self.__lemmatisation()\n",
    "        print(\"Второй этап пройден\")\n",
    "        self.__splitter()\n",
    "        print(\"Третий этап пройден\")\n",
    "        self.__vectorisation()\n",
    "        print(\"Подготовка завершена\")\n",
    "\n",
    "        \n",
    "    def __splitter(self):\n",
    "        \"\"\" Функция, разделяющая обработанные данные на тренировочную,\n",
    "        валидационную и тестовую\n",
    "        \"\"\"\n",
    "\n",
    "        presplited_data = train_test_split(self.lemm_corpus,\n",
    "                                            self.start_frame[self.target_column],\n",
    "                                            test_size = 0.2,random_state = 42)\n",
    "        splited_data_w_val = train_test_split(presplited_data[1],\n",
    "                                            presplited_data[3],\n",
    "                                            test_size = 0.5,random_state = 42)\n",
    "        self.splited_data = [presplited_data[0],splited_data_w_val[0],splited_data_w_val[1],\n",
    "                                 presplited_data[2],splited_data_w_val[2],splited_data_w_val[3]]\n",
    "\n",
    "        \n",
    "    def __lemmatisation(self):\n",
    "        \"\"\"Функция, отвечающая за лемматизацию слов корпуса\n",
    "        \"\"\"\n",
    "        # Инициализация лемматизатора\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        # Лемматизация корпуса\n",
    "        self.lemm_corpus = self.corpus.apply(lambda sentence: \" \".join([lemmatizer.lemmatize(w,\"n\") for w in nltk.word_tokenize(sentence)]))\n",
    "\n",
    "        \n",
    "    def __text_clearning(self):\n",
    "        \"\"\" Функция, отвечающая за очистку корпуса от лишних символов\n",
    "        \"\"\"\n",
    "        # Выделение корпуса для дальнейшего анализа\n",
    "        corpus = self.start_frame[self.data_column]\n",
    "        # Очистка корпуса от лишних символов\n",
    "        self.corpus = corpus.apply(lambda sentence: re.sub(r'[^a-zA-Z]',' ',sentence))\n",
    "        \n",
    "    def __vectorisation(self):\n",
    "        \"\"\"Функция, отвечающая за векторизацию корпуса\n",
    "        \"\"\"\n",
    "        # Создание словаря со словарями, которые хранят в себе векторизованные данные от разных векторизаторов \n",
    "        self.vect = {str(i()):{} for i in self.solvers}\n",
    "        # Векторизация данных разными методами\n",
    "        for vectorizer in self.solvers:\n",
    "            # Инициализация векторизатора и установка стоп-слов\n",
    "            vector = vectorizer(stop_words = self.stop_words)\n",
    "            # Обучение и трансформация на обучающей выборке\n",
    "            self.vect[str(vectorizer())]['train_data'] = vector.fit_transform(self.splited_data[0])\n",
    "            self.vect[str(vectorizer())]['train_target'] = self.splited_data[3]\n",
    "            # Трансформация тестовой выборки\n",
    "            self.vect[str(vectorizer())]['test_data'] = vector.transform(self.splited_data[1])\n",
    "            self.vect[str(vectorizer())]['test_target'] = self.splited_data[4]\n",
    "            # Трансформация валидационной выборки\n",
    "            self.vect[str(vectorizer())]['valid_data'] = vector.transform(self.splited_data[2])\n",
    "            self.vect[str(vectorizer())]['valid_target'] = self.splited_data[5]\n",
    "            \n",
    "            \n",
    "    def fit(self):\n",
    "        \"\"\"Тренируем все переданные модели\n",
    "        \"\"\"\n",
    "        # Инициализация словаря\n",
    "        self.black_boxes = {str(name):{} for name,_ in self.models_and_params}\n",
    "        # Перебор всех моделей\n",
    "        for model, params in tqdm(self.models_and_params):\n",
    "            # Перебор всех векторизаторов\n",
    "            for vectorizer, data in tqdm(self.vect.items(),desc = str(model)):\n",
    "                # Инициализация внутреннего словаря\n",
    "                self.black_boxes[str(model)][str(vectorizer)] = {}\n",
    "                # Инициализация грида\n",
    "                self.black_boxes[str(model)][str(vectorizer)][\"grid_object\"] = GridSearchCV(model,params,cv = 3,scoring=self.score)\n",
    "                # Тренировка грида\n",
    "                self.black_boxes[str(model)][str(vectorizer)][\"grid_object\"].fit(self.vect[str(vectorizer)]['train_data'],\n",
    "                                                                            self.vect[str(vectorizer)]['train_target'])\n",
    "                # Сохранение лучшей модели\n",
    "                self.black_boxes[str(model)][str(vectorizer)][\"best_model\"] = self.black_boxes[str(model)][str(vectorizer)][\"grid_object\"].best_estimator_\n",
    "                # Сохранение лучшего скора на разных выборках\n",
    "                self.black_boxes[str(model)][str(vectorizer)][\"best_score_train\"] = self.black_boxes[str(model)][str(vectorizer)][\"grid_object\"].best_score_\n",
    "                self.black_boxes[str(model)][str(vectorizer)][\"best_score_valid\"] = f1_score(self.vect[str(vectorizer)]['valid_target'],self.black_boxes[str(model)][str(vectorizer)][\"best_model\"].predict(self.vect[str(vectorizer)]['valid_data']))\n",
    "                self.black_boxes[str(model)][str(vectorizer)][\"best_score_test\"] = f1_score(self.vect[str(vectorizer)]['test_target'],self.black_boxes[str(model)][str(vectorizer)][\"best_model\"].predict(self.vect[str(vectorizer)]['test_data']))\n",
    "                # Поиск максимального скора на валидационной выборке\n",
    "                if self.black_boxes[str(model)][str(vectorizer)][\"best_score_valid\"] > self.max_score:\n",
    "                    self.max_score = self.black_boxes[str(model)][str(vectorizer)][\"best_score_test\"]\n",
    "                    self.best_model = self.black_boxes[str(model)][str(vectorizer)][\"best_model\"]\n",
    "                    \n",
    "        return {\"max_score\":self.max_score,\"best_model\":self.best_model}\n",
    "    \n",
    "    def get_info(self):\n",
    "        \"\"\" Функция, возвращающая всю собранную информацию об обучении\n",
    "        \"\"\"\n",
    "        return self.black_boxes\n",
    "            \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Инициализация параметров\n",
    "params_Log = {\"max_iter\":[1000,2000,100]}\n",
    "params_RF = {\"n_estimators\":[40,200,20],\"max_depth\":[2,10]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Первый этап пройден\n",
      "Второй этап пройден\n",
      "Третий этап пройден\n",
      "Подготовка завершена\n"
     ]
    }
   ],
   "source": [
    "# Инициализация класса\n",
    "cl = toxic_classification([(LogisticRegression(random_state = 42,class_weight='balanced',n_jobs = -1),params_Log),\n",
    "                           (RandomForestClassifier(random_state = 42,class_weight='balanced',n_jobs = -1),params_RF)],\n",
    "                          'f1',[TfidfVectorizer,CountVectorizer],stop_words,frame,'toxic',\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      "LogisticRegression(class_weight='balanced', n_jobs=-1, random_state=42):   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "LogisticRegression(class_weight='balanced', n_jobs=-1, random_state=42):  50%|█████     | 1/2 [00:25<00:25, 25.99s/it]\u001b[A\n",
      "LogisticRegression(class_weight='balanced', n_jobs=-1, random_state=42): 100%|██████████| 2/2 [01:52<00:00, 56.24s/it]\u001b[A\n",
      " 50%|█████     | 1/2 [01:52<01:52, 112.47s/it]\n",
      "RandomForestClassifier(class_weight='balanced', n_jobs=-1, random_state=42):   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "RandomForestClassifier(class_weight='balanced', n_jobs=-1, random_state=42):  50%|█████     | 1/2 [00:34<00:34, 34.40s/it]\u001b[A\n",
      "RandomForestClassifier(class_weight='balanced', n_jobs=-1, random_state=42): 100%|██████████| 2/2 [01:07<00:00, 33.85s/it]\u001b[A\n",
      "100%|██████████| 2/2 [03:00<00:00, 90.09s/it] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'max_score': 0.758893280632411,\n",
       " 'best_model': LogisticRegression(class_weight='balanced', max_iter=1000, n_jobs=-1,\n",
       "                    random_state=42)}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cl.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Вывод\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Необходимая модель найдена, скор полученный на тестовой выборке удовлетворяет условию, в данных не найдены пропуски и дубликаты. Лучшая модель основанна на алгоритме LogisticRegression\n",
    "\n",
    "- Полученный скор: 0.758"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
